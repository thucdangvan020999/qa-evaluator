{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Context</th>\n",
       "      <th>Predicted Answers</th>\n",
       "      <th>Actual Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is 1+1</td>\n",
       "      <td>1+1=2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is 1+2</td>\n",
       "      <td>1+2=3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Questions Context  Predicted Answers  Actual Answers\n",
       "0  what is 1+1   1+1=2                2.0             2.0\n",
       "1  what is 1+2   1+2=3                2.0             3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./testing.csv').dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Anthropic\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"Cleans text extracted from a PDF file.\"\"\"\n",
    "    # TODO: Remove References/Bibliography section.\n",
    "    return remove_citations(text)\n",
    "\n",
    "\n",
    "def remove_citations(text: str) -> str:\n",
    "    \"\"\"Removes in-text citations from a string.\"\"\"\n",
    "    # (Author, Year)\n",
    "    text = re.sub(r'\\([A-Za-z0-9,.\\s]+\\s\\d{4}\\)', '', text)\n",
    "    # [1], [2], [3-5], [3, 33, 49, 51]\n",
    "    text = re.sub(r'\\[[0-9,-]+(,\\s[0-9,-]+)*\\]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "And explain why the STUDENT ANSWER is correct or incorrect.\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "You are also asked to identify potential sources of bias in the question and in the true answer.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "And explain why the STUDENT ANSWER is correct or incorrect, identify potential sources of bias in the QUESTION, and identify potential sources of bias in the TRUE ANSWER.\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT_BIAS_CHECK = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "template = \"\"\"You are assessing a submitted student answer to a question relative to the true answer based on the provided criteria: \n",
    "    \n",
    "    ***\n",
    "    QUESTION: {query}\n",
    "    ***\n",
    "    STUDENT ANSWER: {result}\n",
    "    ***\n",
    "    TRUE ANSWER: {answer}\n",
    "    ***\n",
    "    Criteria: \n",
    "      relevance:  Is the submission referring to a real quote from the text?\"\n",
    "      conciseness:  Is the answer concise and to the point?\"\n",
    "      correct: Is the answer correct?\"\n",
    "    ***\n",
    "    Does the submission meet the criterion? First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print the \"CORRECT\" or \"INCORRECT\" (without quotes or punctuation) on its own line corresponding to the correct answer.\n",
    "    Reasoning:\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT_OPENAI = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT_FAST = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Decide if the following retrieved context is relevant: \\n\n",
    "    {result}\n",
    "    Answer in the following format: \\n\n",
    "    \"Context is relevant: True or False.\" \\n \n",
    "    And explain why it supports or does not support the correct answer: {answer}\"\"\"\n",
    "\n",
    "GRADE_DOCS_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Decide if the following retrieved context is relevant to the {answer}: \\n\n",
    "    {result}\n",
    "    Answer in the following format: \\n\n",
    "    \"Context is relevant: True or False.\" \\n \"\"\"\n",
    "\n",
    "GRADE_DOCS_PROMPT_FAST = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_model_answer(predicted_dataset: List, predictions: List, grade_answer_prompt: str) -> List:\n",
    "    \"\"\"\n",
    "    Grades the distilled answer based on ground truth and model predictions.\n",
    "    @param predicted_dataset: A list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: A list of dictionaries containing model predictions for the questions.\n",
    "    @param grade_answer_prompt: The prompt level for the grading. Either \"Fast\" or \"Full\".\n",
    "    @return: A list of scores for the distilled answers.\n",
    "    \"\"\"\n",
    "    # Grade the distilled answer\n",
    "    st.info(\"`Grading model answer ...`\")\n",
    "    # Set the grading prompt based on the grade_answer_prompt parameter\n",
    "    if grade_answer_prompt == \"Fast\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_FAST\n",
    "    elif grade_answer_prompt == \"Descriptive w/ bias check\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_BIAS_CHECK\n",
    "    elif grade_answer_prompt == \"OpenAI grading prompt\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_OPENAI\n",
    "    else:\n",
    "        prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-16k-0613\", temperature=0),\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        predicted_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "\n",
    "    return graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def grade_model_retrieval(gt_dataset: List, predictions: List, grade_docs_prompt: str):\n",
    "    \"\"\"\n",
    "    Grades the relevance of retrieved documents based on ground truth and model predictions.\n",
    "    @param gt_dataset: list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: list of dictionaries containing model predictions for the questions\n",
    "    @param grade_docs_prompt: prompt level for the grading. Either \"Fast\" or \"Full\"\n",
    "    @return: list of scores for the retrieved documents.\n",
    "    \"\"\"\n",
    "    # Grade the docs retrieval\n",
    "\n",
    "    # Set the grading prompt based on the grade_docs_prompt parameter\n",
    "    prompt = GRADE_DOCS_PROMPT_FAST if grade_docs_prompt == \"Fast\" else GRADE_DOCS_PROMPT\n",
    "\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-16k-0613\", temperature=0),\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        gt_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "    return graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_model_answer(predicted_dataset: List, predictions: List, grade_answer_prompt: str) -> List:\n",
    "    \"\"\"\n",
    "    Grades the distilled answer based on ground truth and model predictions.\n",
    "    @param predicted_dataset: A list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: A list of dictionaries containing model predictions for the questions.\n",
    "    @param grade_answer_prompt: The prompt level for the grading. Either \"Fast\" or \"Full\".\n",
    "    @return: A list of scores for the distilled answers.\n",
    "    \"\"\"\n",
    "    # Grade the distilled answer\n",
    "    # Set the grading prompt based on the grade_answer_prompt parameter\n",
    "    if grade_answer_prompt == \"Fast\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_FAST\n",
    "    elif grade_answer_prompt == \"Descriptive w/ bias check\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_BIAS_CHECK\n",
    "    elif grade_answer_prompt == \"OpenAI grading prompt\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_OPENAI\n",
    "    else:\n",
    "        prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-16k-0613\", temperature=0),\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        predicted_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "\n",
    "    return graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"Which company sold the microcomputer kit that his friend built himself?\", 'answer' : 'Healthkit'},\n",
    "    {'question' : \"What was the small city he talked about in the city that is the financial capital of USA?\", 'answer' : 'Yorkville, NY'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [{'question': 'Which company sold the microcomputer kit that his friend built himself?',\n",
    "  'answer': 'Healthkit',\n",
    "  'result': ' The microcomputer kit was sold by Heathkit.'},\n",
    " {'question': 'What was the small city he talked about in the city that is the financial capital of USA?',\n",
    "  'answer': 'Yorkville, NY',\n",
    "  'result': ' The small city he talked about is New York City, which is the financial capital of the United States.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_grade = grade_model_answer(question_answers, predictions, GRADE_ANSWER_PROMPT_FAST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: CORRECT\\n\\nThe STUDENT ANSWER is correct because it accurately identifies Heathkit as the company that sold the microcomputer kit. Although there is a minor difference in spelling (Heathkit vs. Healthkit), this does not affect the factual accuracy of the answer.'},\n",
       " {'results': 'GRADE: INCORRECT\\n\\nThe STUDENT ANSWER is incorrect because the true answer is \"Yorkville, NY\" and not \"New York City.\" The student\\'s answer contains conflicting statements as it states that the small city he talked about is New York City, which is not true.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_retrieve = [{'question': 'Which company sold the microcomputer kit that his friend built himself?',\n",
    "  'answer': 'Healthkit',\n",
    "  'result': ' The microcomputer kit was sold by Heathkit.'},\n",
    " {'question': 'What was the small city he talked about in the city that is the financial capital of USA?',\n",
    "  'answer': 'Yorkville, NY',\n",
    "  'result': ' The small city he talked about is New York City, which is the financial capital of the United States.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_grade = grade_model_retrieval(question_answers, predictions_retrieve, GRADE_DOCS_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'Context is relevant: True\\n\\nExplanation: The retrieved context directly answers the question by stating that the microcomputer kit was sold by Heathkit. This information supports the correct answer to the question.'},\n",
       " {'results': 'Context is relevant: False.\\n\\nThe retrieved context mentions that the small city he talked about is New York City, which is the financial capital of the United States. However, the question specifically asks for the small city mentioned within the financial capital of the USA. The retrieved context does not provide any information about a small city within New York City, so it does not support the correct answer.'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Context</th>\n",
       "      <th>Predicted Answers</th>\n",
       "      <th>Actual Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is 1+1</td>\n",
       "      <td>1+1=2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is 1+2</td>\n",
       "      <td>1+2=3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is 1+3</td>\n",
       "      <td>1+3=4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is 1+4</td>\n",
       "      <td>1+4=5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is 1+5</td>\n",
       "      <td>1+5=6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Questions Context  Predicted Answers  Actual Answers\n",
       "0  what is 1+1   1+1=2                2.0             2.0\n",
       "1  what is 1+2   1+2=3                2.0             3.0\n",
       "2  what is 1+3   1+3=4                2.0             4.0\n",
       "3  what is 1+4   1+4=5                2.0             5.0\n",
       "4  what is 1+5   1+5=6                2.0             6.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./testing.csv').dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'what is 1+1', 'answer': 2.0, 'result': 2.0},\n",
       " {'question': 'what is 1+2', 'answer': 3.0, 'result': 2.0},\n",
       " {'question': 'what is 1+3', 'answer': 4.0, 'result': 2.0},\n",
       " {'question': 'what is 1+4', 'answer': 5.0, 'result': 2.0},\n",
       " {'question': 'what is 1+5', 'answer': 6.0, 'result': 2.0}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_qa = df.apply(lambda x: {\n",
    "    'question': x['Questions'],\n",
    "    'answer': x['Actual Answers'],\n",
    "    'result': x['Predicted Answers']\n",
    "}, axis=1).tolist()\n",
    "result_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc = df.apply(lambda x: {\n",
    "    'question': x['Questions'],\n",
    "    'answer': x['Actual Answers'],\n",
    "    'result': x['Context']\n",
    "}, axis=1).tolist()\n",
    "result_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
